\section{General Discussion and Conclusions}
\label{sec:discussion}

Except for the BREQ dataset, a revisited version of a DP algorithm from 1966 (Revisited Terminating Step-Off, R-TSO) had lower mean times than the only known implementation of the current state-of-the-art algorithm (EDUK2 /PYAsUKP).
Such results bring up two central questions the authors address: Why TSO was not considered in recent comparisons? How R-TSO outperformed the current state-of-the-art after five decades of study of the UKP?

\begin{table}[H]
\label{tab:one_table}
\def\arraystretch{1.1}
%\setlength\tabcolsep{4px}

\begin{adjustbox}{max width=\textwidth, center}
\begin{tabular}{@{\extracolsep{4pt}}lrrrrrrrr@{}}

& \multicolumn{2}{c}{PYAsUKP\textsuperscript{a}} & \multicolumn{2}{c}{RR} & \multicolumn{2}{c}{BREQ} & \multicolumn{2}{c}{CSP\textsuperscript{b}}\\
\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}
Method & \multicolumn{1}{c}{fin} & \multicolumn{1}{c}{avg} & \multicolumn{1}{c}{fin} & \multicolumn{1}{c}{avg} & \multicolumn{1}{c}{fin} & \multicolumn{1}{c}{avg} & \multicolumn{1}{c}{fin} & \multicolumn{1}{c}{avg}\\
%& fin & time & fin & time & fin & time & fin & time\\
\cline{1-1}\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}
<<one_table,results='asis'>>=
library(dplyr)

pya_csv2 <- read.csv(
  '../data/pya_ukp5.csv', sep = ";", stringsAsFactors = FALSE
) %>% filter(algorithm != 'fmtu1' & algorithm != 'fmtu2')
pya_csv2$X <- NULL

# Get only the algorithms executed over the reduced pyasukp dataset.
reduced_pya <- read.csv(
  '../data/mtus_pya.csv', sep = ";", stringsAsFactors = FALSE
) %>% filter(algorithm != 'fmtu1' & algorithm != 'fmtu2')
reduced_pya$X <- NULL
reduced_pya <- rbind(
  reduced_pya,
  filter(pya_csv2, algorithm == 'cplex2nt' | algorithm == 'gurobi0nt')
)
# We need to get the runs of OSO for the reduced pyasukp dataset to
# have one algorithm with the correct instance values for all instances.
reduced_pya <- rbind(
  reduced_pya,
  semi_join(
    filter(pya_csv2, algorithm == 'ordered_step_off'),
    reduced_pya,
    by = 'filename'
  )
)

clean_and_summarise <- function(dt, correct_alg = 'none') {
  qt_inst <- length(unique(dt$filename))
  has_opt <- correct_alg != 'none'
  
  # Creates a column with the correct opt.
  if (has_opt) {
    correct_opt <- dt %>% filter(algorithm == correct_alg) %>%
      select(filename, c_opt = opt)
    dt <- inner_join(dt, correct_opt, by = 'filename')
  }
  # Set internal_time to NA for instances in which Gurobi finished because
  # memory exhaustion (opt == 0), or instances for which Gurobi had wrong
  # results (abs(opt - c_opt)/opt > 1e-7), or instances for which
  # cplex or gurobi hit the timeout (internal_time > 1799).
  if (has_opt) {
    dt <- dt %>% mutate(
      internal_time = ifelse(
        (is.na(opt)                    |
           opt == 0                    |
           abs(opt - c_opt)/opt > 1e-7 |
           internal_time > 1799),
        NA,
        internal_time
      )
    )
  } else {
    dt <- dt %>% mutate(
      internal_time = ifelse(internal_time > 1799, NA, internal_time)
    )
  }
  
  dt <- dt %>% select(algorithm, internal_time)
  
  cols <- dt %>% group_by(algorithm) %>% summarise(
    finished = sprintf('%d', qt_inst - sum(is.na(internal_time))),
    mean_time = sprintf('%.2f', mean(internal_time, na.rm = T))
  )
  
  return(cols)
}

red_cols <- clean_and_summarise(reduced_pya, 'ordered_step_off') %>%
  filter(algorithm != 'ordered_step_off')
fast_cols <- clean_and_summarise(filter(
  pya_csv2, algorithm != 'cplex2nt' & algorithm != 'gurobi0nt'
))
pya_cols <- rbind(red_cols, fast_cols)
pya_cols$algorithm <- recode(pya_cols$algorithm, pyasukp = 'eduk2')

rr_csv2 <- read.csv(
  '../data/realistic_random.csv', sep = ";", stringsAsFactors = FALSE
)
rr_cols <- clean_and_summarise(rr_csv2, 'ordered_step_off')

breq_csv2 <- read.csv(
  '../data/128_16_std_breqd.csv', sep = ";", stringsAsFactors = FALSE
)
breq_cols <- clean_and_summarise(breq_csv2, 'cpp-mtu2')

csp_csv2 <- read.csv(
  '../data/csp_6195.csv', sep = ";", stringsAsFactors = FALSE
) %>% filter(
  algorithm == 'cplex_cutstock' |
  algorithm == 'mtu1_cutstock'  |
  algorithm == 'ordso_int_ns'   |
  algorithm == 'terso_int'
) %>% mutate(
  internal_time = hex_sum_knapsack_time + hex_sum_sort_time
) %>% select(algorithm, filename, internal_time)
csp_cols <- clean_and_summarise(csp_csv2)
csp_cols$algorithm <- recode(csp_cols$algorithm,
  # not really cplex2nt, but it is cplex, and this is the code
  # we refer to it in the other csvs
  cplex_cutstock = 'cplex2nt',
  mtu1_cutstock = 'cpp-mtu1',
  ordso_int_ns = 'ordered_step_off',
  terso_int = 'terminating_step_off'
)

create_empty_rows <- function (existing, universe) {
  forgotten <- vector(mode = 'character')
  for (u in universe) {
    if (!(u %in% existing)) { forgotten <- append(u, forgotten) }
  }
  return(data.frame(
    algorithm = forgotten,
    finished = '--',
    mean_time = '--'
  ))
}

add_missing <- function (dt) {
  all_algs = c(
    'cpp-mtu1', 'cpp-mtu2', 'cplex2nt', 'gurobi0nt', 'ordered_step_off',
    'terminating_step_off', 'mgreendp', 'eduk', 'eduk2'
  )
  mock <- create_empty_rows(dt$algorithm, all_algs)
  return(rbind(dt, mock))
}

pya_cols <- add_missing(pya_cols) %>% rename(
  pya_fin = finished,
  pya_mtm = mean_time
)
rr_cols <- add_missing(rr_cols) %>% rename(
  rr_fin = finished,
  rr_mtm = mean_time
)
breq_cols <- add_missing(breq_cols) %>% rename(
  breq_fin = finished,
  breq_mtm = mean_time
)
csp_cols <- add_missing(csp_cols) %>% rename(
  csp_fin = finished,
  csp_mtm = mean_time
)

one_table <- inner_join(pya_cols, rr_cols, by = 'algorithm')
one_table <- inner_join(one_table, breq_cols, by = 'algorithm')
one_table <- inner_join(one_table, csp_cols, by = 'algorithm')
one_table$algorithm <- recode(
  one_table$algorithm,
  eduk  = 'EDUK',
  eduk2 = 'EDUK2',
  `cpp-mtu1` = 'MTU1',
  `cpp-mtu2` = 'MTU2',
  gurobi0nt = 'Gurobi',
  cplex2nt = 'CPLEX',
  ordered_step_off = 'R-OSO',
  terminating_step_off = 'R-TSO',
  mgreendp = 'R-GFDP'
)
one_table$algorithm <- factor(
  one_table$algorithm,
  levels = c(
    'R-TSO', 'R-OSO', 'R-GFDP', 'EDUK2', 'EDUK',
    'MTU2', 'MTU1', 'CPLEX', 'Gurobi'
  )
)
one_table <- one_table %>% arrange(algorithm)

library(xtable)

print.xtable(
  xtable(one_table),
  only.contents = T,
  include.rownames = F,
  include.colnames = F,
  hline.after = NULL,
  comment = F
)
@
\hline
\end{tabular}
\end{adjustbox}
%\caption{Time (in seconds) for the PYAsUKP 4540 Instances (see Section \ref{sec:pya_exp}). Columns \textbf{n} and \(w_{min}\) values must be multiplied by \(10^3\) to obtain their true value. Let \(T\) be the set of run times reported by the R-TSO, GFDP or EDUK2 for the instance dataset described by a row. The meaning of the columns \textbf{avg}, \textbf{sd} and \textbf{max}, is, respectively, the arithmetic mean of \(T\), the standard deviation of \(T\), the maximum value of \(T\). For the subset-sum instances, there are ten instances for each possible combination of: \(w_{min} \in \{10^3, 5\times10^3, 10^4, 5\times10^4, 10^5\}\); \(w_{max} \in \{5\times10^5, 10^6\}\) and \(n \in \{10^3, 2\times10^3, 5\times10^3, 10^4\}\), totaling 400 instances.}
\caption{
The number of \textbf{fin}ished runs of a method (row) over a dataset (column), and the \textbf{av}era\textbf{g}e time (in seconds) of the finished runs (dashes mean the respective method was not used to solve the respective dataset).
%The runs not finished are mostly timeouts (all runs had a half hour time limit), but some (as the Gurobi ones) include memory exhaustion and wrong results.
%The dashes mean the respective method was not executed over the respective dataset.
\textsuperscript{a} The MTU1, MTU2, CPLEX, and Gurobi were executed only over the 454 instances of the reduced PYAsUKP dataset (not all 4540 instances as the remaining methods).
\textsuperscript{b} The times reported are for the following variants: R-TSO -- integer profit, sort by efficiency; R-OSO -- integer profit, sort by weight; CPLEX -- custom code reusing the model and only changing the profits (coefficients of the objective function) between the iterations. %In the CSP dataset, the 6192 instances are CSP instances, the run is considered finished if the CSP was solved to proven optimality before the time limit, and the time is the mean time spent by instance solving all their pricing problems with the respective method. Also in the CSP column: the TSO results come from the variant that converts profit to integers and sort items by efficiency; the OSO variant also converts profit to integers but sort by weight; the CPLEX results come from a custom code that reuses the same model and only changes the profits at each iteration. 
}
\end{table}

The authors believe TSO was not considered in recent comparisons because: the algorithm was very similar to the naïve DP for the UKP but the authors did not emphasize it was orders of magnitude faster; B\&B algorithms performed better than TSO in uncorrelated and weakly correlated instances with one hundred items.
As far as the authors know, the proposal of MTU1 was the last time TSO was included in a comparison~\citep{mtu1}.
In the experiments presented in~\cite{mtu1}, TSO was about four times slower than MTU1 in the instances with~\(n = 25\); about two or three times slower in the instances with~\(n = 50\); and less than two times slower in instances with~\(n = 100\).
Such instances are now too small to consider, and the relative difference between TSO and MTU1 mean times was less than one order of magnitude apart and diminishing.
This trend hinted the possibility of the times taken by OSO/TSO and MTU1 converging (or even OSO/TSO taking less time than MTU1) for larger instances (e.g., OSO/TSO algorithm could have a costly initialization process but a better average-case complexity).

% Also, the trend observed could indicate that for instances with a greater \(n\) value, OSO/TSO algorithm would have lower times than MTU1, as their relative difference was diminishing. %(e.g., OSO/TSO algorithm could have a costly initialization process but a better average-case complexity).

\subsection{An algorithm is dominated by other in the context of a dataset}

The comparisons often found in the literature: 1) only compared a newly proposed algorithm to the algorithm that `won' the last comparison; 2) proposed new artificial datasets based on some definition of being `harder to solve'.
These two characteristics led to the following scenario: algorithm B dominates algorithm A in the context of dataset D1; algorithm C dominates algorithm B in the context of dataset D2; nothing guarantees that algorithm A does not dominate algorithm C in the context of dataset D2, as algorithm A was not included in the last comparison.
%The newly proposed dataset is often harder to solve because it has characteristics that made it harder to solve by some algorithm in the comparison (often the `loser').
%It is not considered if the characteristics that make it harder also negatively affect the algorithms excluded from the comparison, or if they are algorithm or approach-specific.

A concrete example of this behavior in the UKP literature follows: MTU2 was compared only to MTU1, and in the context of a new dataset of large \(n\) and rich in simple and multiple dominated items~\citep{mtu2}; EDUK and EDUK2 were compared only to MTU2, and in the context of new datasets with a smaller \(n\) but with less dominated items and higher \(w_{min}\)~\citep{eduk,pya}.
To be fair, the objective of these papers was to show how the newly proposed algorithm was not negatively impacted by some instance characteristics as the older algorithm was.
However, in such experiments, no old competitive algorithm of the same solving approach as the newly proposed algorithm (DP or B\&B) was included.

The purpose of the BREQ item distribution is to further illustrate how artificial datasets that favor one approach over another can be created.
The BREQ instances are hard to solve by DP algorithms and easy to solve by B\&B algorithms.
If they were the only instances considered, then MTU2 would be considered the best algorithm. However, considering all remaining datasets, MTU2 often presents the worst time performance.
The BREQ instances do not suffer from the same richness of simple, multiple and collective dominated items that led uncorrelated instances to be criticized and abandoned.
However, threshold dominance is widespread in BREQ instances and, as far the authors know, no real-world instances follow the BREQ distribution.%, so the authors do not suggest their use in future experiments.
%The authors ask any future researchers to take in account the purpose for which BREQ instances were created (i.e., favor the B\&B approach) before including them in their experiments.

The effects of artificial instances in shaping what is considered the best algorithms for UKP is not limited to the instances proposed for the UKP.
\cite{irnich} created the GI instances to be harder to solve by their column generation implementation: 
``[...] generated new and harder CS instances. These are characterized by huge values for the capacity (to complicate the subproblems) and larger numbers of items with distinct lengths.''.
Their implementation used a DP algorithm to solve pricing problems.
The characteristics of these newly proposed BPP/CSP instances made the pricing problems harder to solve by a DP algorithm, but not necessarily by a B\&B algorithm, which is less affected by parameters like the knapsack capacity.
To evaluate which is the best UKP algorithm to solve pricing subproblems, the BPP/CSP instances had also to be representative of real-world instances; otherwise, another layer of bias is laid.
The ANI\&AI instances, which MTU1 had difficulties to solve the pricing problems, are also instances created with the purpose to be hard to solve~\citep{survey2014}.
The definition of hard to solve is different between the GI and ANI\&AI instances, as in ANI\&AI instances the objective is to make B\&B algorithms for the BPP/CSP struggle to prove the optimality of a solution for a BPP/CSP instance.
The way such characteristic makes the pricing problems from ANI\&AI instances harder to solve by MTU1 is not so clear as in the case of GI instances and DP algorithms.

\subsection{Comments on R-TSO and EDUK2 times gap}

The authors believe many factors allowed R-TSO to outperform EDUK2 PYAsUKP implementation.
Some of them are: the weak solution dominance implicitly applied by (R-)TSO seems to be as effective as the explicit simple, multiple, collective and threshold dominance applied by EDUK2; R-TSO has better space locality; R-TSO solution backtrack trades memory for time (while EDUK does the opposite).

%The authors did not try to directly compare the effectiveness of EDUK2 and R-TSO dominance approach, as such would need changes in EDUK2 code to allow gathering extra data.
%However, as R-TSO finishes early if only the best item could be added to the solution, this means that TSO can discard every other item by applying solution dominance, as EDUK2 does.

EDUK2 PYAsUKP implementation uses lazy lists to store solutions, while R-TSO uses an array.
A strongly correlated  instance (\(\alpha = -5\), \(n = 10000\), \(w_{min} = 110000\), \(c = 9008057\)) of the PYAsUKP dataset was the one EDUK2 spent more time to solve (416 seconds, R-TSO spent about one second to solve the same instance).
By the use of the \emph{perf} profiler, it was possible to verify that EDUK2 PYAsUKP implementation executed about \(1122\) instructions per cache miss, while R-TSO executed about \(288653\) instructions per cache miss.
The performance gain for using an array-based implementation was also observed in~\cite[p.~19]{irnich}, which tried to follow the approach suggested by EDUK in their pricing problem solver: ``In contrast, for UKP we found that a straightforward array-based implementation of the DP approach is faster than the list-based approach. We suspect that on a modern CPU, the smaller state graph of UKP can be accessed much faster (due to caching techniques) so that the solution of the UKP subproblems as they occur in the BP benchmark instances is possible in almost no (measurable) time.''.

%In~\cite{eduk}, it is said that EDUK retrieves all optimal solutions for an UKP instance.
%To do the same, R-TSO could not make use of the tiebreaking optimization mentioned in \autoref{sec:oso_and_sol_dom}, what would affect its performance considerably in strongly correlated and subset-sum datasets.
%Also, the application of the solution dominance would need to be less strict as, for example, for two items \(j\) and \(i\) that respect \(w_i < w_j\) and \(p_i = p_j\), \(j\) can yet be present in an optimal solution.

\subsection{Conclusions}

In conclusion,
i) the choice of artificial instance datasets had an important role defining which algorithms were considered the best by the literature; 
ii) the simple, multiple, collective, and threshold dominance relations can be generalized to solution dominance, and the application of a weak version of it shows similar efficiency;
iii) there is evidence that the items distribution of pricing problems can converge to a strongly correlated distribution (which can take exponential time to solve by B\&B);
iv) there is evidence that converting the profit of pricing problems to large integers do not cause significant loss to the master problem objetive value;
v) the development of new and tighter periodicity bounds is of little use to the improvement of the state-of-the-art algorithms for UKP;
vi) CPLEX has better performance than B\&B algorithms for UKP in instances with few items but `hard' to solve by B\&B approach, but it is worse when the instances are large but `easy' (many variables) or too many small instances (costly initialization).

%While the following quote was written in the context of the 0-1 KP, the author found it relevant to complement what was just said: ``Dynamic programming is one of our best approaches for solving difficult (KP), since this is the only solution method which gives us a worst-case guarantee on the running time, independently on whether the upper bounding tests will work well.''~\cite[p.~13]{where_hard}.

\subsection{Future work}
\label{sec:future_works}

Many questions raised during the development of this paper ended up unanswered:% The authors selected they found more interesting in the list below.

\begin{itemize}
\item How similar are the datasets of the UKP and the BPP/CSP presented in the literature to the ones existent in the real world? Do the instances found in the real-world favor some approaches over others?
\item If a B\&B phase was added to the terminating step-off (as in EDUK2), and a C++ and array-based implementation of EDUK2 was written, would they have a similar performance?
%\item What would be the practical performance of an implementation of the algorithm described in~\cite{babayev}?
%\item How do the traits of the optimal solution for the pricing subproblem affect the master problem? Does always returning an optimal solution with minimal weight has a negative effect? What about adding all patterns that improve the master problem solution, and not only the best pattern (i.e., an optimal solution)? Or, instead, adding all optimal solutions instead of only one? %\item Are the profit values (and, consequently, the items distributions) of the pricing subproblems uniform between similar BPP/CSP instances, or the same BPP/CSP instance, or in both cases? Is it possible that they converge to a specific distribution at each iteration of the column generation?
\item The difference in performance of MTU1/MTU2 and CPLEX/Gurobi over the PYAsUKP dataset comes from the fact MTU1/MTU2 are depth-first? What would be the performance of a best-bound B\&B made for the UKP?
\end{itemize}

\subsection{Acknowledgements}

The authors are thankful to the CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico) and FAPERGS (Fundação de Amparo à pesquisa do Estado do Rio Grande do Sul) for the financial support. The authors are also thankful to Vincent Poirriez for his help with PYAsUKP.

